{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijishmadhavan/Lama-restore/blob/master/LaMa_restore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwXRMaNHW4r5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install requirements\n",
        "%%capture\n",
        "\n",
        "!git clone https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life.git photo_restoration\n",
        "%cd photo_restoration/Face_Enhancement/models/networks\n",
        "!git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
        "!cp -rf Synchronized-BatchNorm-PyTorch/sync_batchnorm .\n",
        "%cd ../../../\n",
        "%cd Global/detection_models\n",
        "!git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
        "!cp -rf Synchronized-BatchNorm-PyTorch/sync_batchnorm .\n",
        "%cd ../../\n",
        "%cd /content/photo_restoration/Global\n",
        "import os\n",
        "isExist = os.path.exists(\"checkpoints/detection/\")\n",
        "if not isExist:\n",
        "  os.makedirs('checkpoints/detection/')\n",
        "\n",
        "\n",
        "!wget https://www.dropbox.com/s/wbwtopeevk6vnwe/FT_Epoch_latest.pt -P checkpoints/detection/\n",
        "%cd /content/photo_restoration\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/saic-mdal/lama.git\n",
        "%cd /content/photo_restoration/lama\n",
        "\n",
        "\n",
        "!pip install torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 torchtext==0.9\n",
        "!pip install -r requirements.txt --quiet\n",
        "!pip install wget --quiet\n",
        "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html --quiet\n",
        "\n",
        "isExist = os.path.exists(\"big-lama/models\")\n",
        "if not isExist:\n",
        "  os.makedirs('big-lama/models')\n",
        "%cd /content/photo_restoration/lama/big-lama/models\n",
        "os.system(\"wget https://huggingface.co/akhaliq/lama/resolve/main/best.ckpt\")\n",
        "%cd /content/photo_restoration/lama/big-lama\n",
        "\n",
        "!wget \"https://www.dropbox.com/s/tihpgx1r44utu7a/config.yaml\" \n",
        "\n",
        "!pip uninstall opencv-python-headless -y --quiet\n",
        "!pip install opencv-python-headless==4.1.2.30 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload Image\n",
        "\n",
        "\n",
        "%cd /content\n",
        "from google.colab import files\n",
        "files = files.upload()\n",
        "name = list(files.keys())[0]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-YGrYto4ORTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Detect scratches\n",
        "%cd /content/photo_restoration/Global\n",
        "import gc\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision as tv\n",
        "from PIL import Image, ImageFile\n",
        "\n",
        "from detection_models import networks\n",
        "from detection_util.util import *\n",
        "\n",
        "\n",
        "\n",
        "def data_transforms(img, full_size, method=Image.BICUBIC):\n",
        "    if full_size == \"full_size\":\n",
        "        ow, oh = img.size\n",
        "        h = int(round(oh / 16) * 16)\n",
        "        w = int(round(ow / 16) * 16)\n",
        "        if (h == oh) and (w == ow):\n",
        "            return img\n",
        "        return img.resize((w, h), method)\n",
        "\n",
        "    elif full_size == \"scale_256\":\n",
        "        ow, oh = img.size\n",
        "        pw, ph = ow, oh\n",
        "        if ow < oh:\n",
        "            ow = 256\n",
        "            oh = ph / pw * 256\n",
        "        else:\n",
        "            oh = 256\n",
        "            ow = pw / ph * 256\n",
        "\n",
        "        h = int(round(oh / 16) * 16)\n",
        "        w = int(round(ow / 16) * 16)\n",
        "        if (h == ph) and (w == pw):\n",
        "            return img\n",
        "        return img.resize((w, h), method)\n",
        "\n",
        "\n",
        "def scale_tensor(img_tensor, default_scale=256):\n",
        "    _, _, w, h = img_tensor.shape\n",
        "    if w < h:\n",
        "        ow = default_scale\n",
        "        oh = h / w * default_scale\n",
        "    else:\n",
        "        oh = default_scale\n",
        "        ow = w / h * default_scale\n",
        "\n",
        "    oh = int(round(oh / 16) * 16)\n",
        "    ow = int(round(ow / 16) * 16)\n",
        "\n",
        "    return F.interpolate(img_tensor, [ow, oh], mode=\"bilinear\")\n",
        "\n",
        "\n",
        "def blend_mask(img, mask):\n",
        "\n",
        "    np_img = np.array(img).astype(\"float\")\n",
        "\n",
        "    return Image.fromarray((np_img * (1 - mask) + mask * 255.0).astype(\"uint8\")).convert(\"RGB\")\n",
        "\n",
        "model = networks.UNet(\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    depth=4,\n",
        "    conv_num=2,\n",
        "    wf=6,\n",
        "    padding=True,\n",
        "    batch_norm=True,\n",
        "    up_mode=\"upsample\",\n",
        "    with_tanh=False,\n",
        "    sync_bn=True,\n",
        "    antialiasing=True,\n",
        ")\n",
        "\n",
        "isExist = os.path.exists(\"/content/photo_restoration/lama/data_for_prediction\")\n",
        "if not isExist:\n",
        "  os.makedirs('/content/photo_restoration/lama/data_for_prediction')\n",
        "\n",
        "output_dir = \"/content/photo_restoration/lama/data_for_prediction\"\n",
        "input_dir = \"/content/photo_restoration/lama/data_for_prediction\"\n",
        "## load model\n",
        "#checkpoint_path = os.path.join(os.path.dirname(__file__), \"checkpoints/detection/FT_Epoch_latest.pt\")\n",
        "checkpoint = torch.load(\"checkpoints/detection/FT_Epoch_latest.pt\", map_location=\"cpu\")\n",
        "model.load_state_dict(checkpoint[\"model_state\"])\n",
        "print(\"model weights loaded\")\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "else: \n",
        "    model.cpu()\n",
        "model.eval()\n",
        "\n",
        "image_name =\"/content/\" + name\n",
        "scratch_image = Image.open(image_name).convert(\"RGB\")\n",
        "w, h = scratch_image.size\n",
        "\n",
        "transformed_image_PIL = data_transforms(scratch_image,full_size = \"full_size\")\n",
        "scratch_image = transformed_image_PIL.convert(\"L\")\n",
        "scratch_image = tv.transforms.ToTensor()(scratch_image)\n",
        "scratch_image = tv.transforms.Normalize([0.5], [0.5])(scratch_image)\n",
        "scratch_image = torch.unsqueeze(scratch_image, 0)\n",
        "_, _, ow, oh = scratch_image.shape\n",
        "scratch_image_scale = scale_tensor(scratch_image)\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    scratch_image_scale = scratch_image_scale.to(\"cuda\")\n",
        "else:\n",
        "    scratch_image_scale = scratch_image_scale.cpu()\n",
        "with torch.no_grad():\n",
        "    P = torch.sigmoid(model(scratch_image_scale))\n",
        "\n",
        "P = P.data.cpu()\n",
        "P = F.interpolate(P, [ow, oh], mode=\"nearest\")\n",
        "\n",
        "tv.utils.save_image(\n",
        "    (P >= 0.4).float(),\n",
        "    os.path.join(\n",
        "        output_dir,\n",
        "        image_name[9:-4] + \"_mask\"+ \".png\",\n",
        "    ),\n",
        "    nrow=1,\n",
        "    padding=0,\n",
        "    normalize=True,\n",
        ")\n",
        "transformed_image_PIL.save(os.path.join(output_dir, image_name[9:-4] + \".png\" ))\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "LE0YxhViAkvB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VZWySTMeGDM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Download result\n",
        "%cd /content/photo_restoration/lama\n",
        "import base64, os\n",
        "from IPython.display import HTML, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import wget\n",
        "from shutil import copyfile\n",
        "import shutil\n",
        "\n",
        "fname = output_dir +\"/\"+ image_name[9:-4] + \".png\" \n",
        "\n",
        "print('Run inpainting')\n",
        "if '.jpeg' in fname:\n",
        "  !PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir=/content/output dataset.img_suffix=.jpeg > /dev/null\n",
        "elif '.jpg' in fname:\n",
        "  !PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir=/content/output  dataset.img_suffix=.jpg > /dev/null\n",
        "elif '.png' in fname:\n",
        "  !PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir=/content/output  dataset.img_suffix=.png > /dev/null\n",
        "else:\n",
        "  print(f'Error: unknown suffix .{fname.split(\".\")[-1]} use [.png, .jpeg, .jpg]')\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/output/\"+ image_name[9:-4] + \"_mask\"+ \".png\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hyqFQB9IeC_P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}